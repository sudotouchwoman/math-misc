{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sudotouchwoman/math-misc/blob/main/notebooks/rnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RNN & LSTM implementation. Teterin Nikita, Spring 2022**"
      ],
      "metadata": {
        "id": "qdLgv-7TdGLZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Building character-level model**\n",
        "\n",
        "Training the network to predict words"
      ],
      "metadata": {
        "id": "7Q8v_kDTdl0q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PINVt9J6dB1p"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word = 'ololoasdasddqweqw123456789'\n",
        "# word = 'hello'"
      ],
      "metadata": {
        "id": "-2ntmUypeGnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seed(seed=10):\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "\n",
        "\n",
        "seed()"
      ],
      "metadata": {
        "id": "I-o_SJvoPAeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Dataset and model**"
      ],
      "metadata": {
        "id": "m8q3Xv5BgDQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# the dataset one-hot encodes the letters\n",
        "# and creates an iterator over the word\n",
        "# on each iteration, yield current letter as input\n",
        "# and the next letter as target\n",
        "\n",
        "class WordDataSet:\n",
        "\n",
        "    def __init__(self, word):\n",
        "        self.chars2idx = {}\n",
        "        self.indices  = []\n",
        "        for c in word: \n",
        "            if c not in self.chars2idx:\n",
        "                self.chars2idx[c] = len(self.chars2idx)\n",
        "            self.indices.append(self.chars2idx[c])\n",
        "\n",
        "        self.vec_size = len(self.chars2idx)\n",
        "        self.seq_len  = len(word)\n",
        "\n",
        "    def get_one_hot(self, idx):\n",
        "        x = torch.zeros(self.vec_size)\n",
        "        x[idx] = 1\n",
        "        return x\n",
        "\n",
        "    def __iter__(self):\n",
        "        return zip(self.indices[:-1], self.indices[1:])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.seq_len\n",
        "\n",
        "    def get_char_by_id(self, id):\n",
        "        for c, i in self.chars2idx.items():\n",
        "            if id == i: return c\n",
        "        return None"
      ],
      "metadata": {
        "id": "BS-7RTc9eHYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VanillaRNN(nn.Module):\n",
        "\n",
        "    def __init__(self, in_size=5, hidden_size=3, out_size=5):\n",
        "        super(VanillaRNN, self).__init__()        \n",
        "        self.x2hidden    = nn.Linear(in_features=in_size, out_features=hidden_size)\n",
        "        self.hidden      = nn.Linear(in_features=hidden_size, out_features=hidden_size)\n",
        "        self.activation  = nn.Tanh()\n",
        "        self.outweight   = nn.Linear(in_features=hidden_size, out_features=out_size)\n",
        "\n",
        "    def forward(self, x, prev_hidden, activate=True):\n",
        "        hidden = self.x2hidden(x) + self.hidden(prev_hidden)\n",
        "        # if one omits activation function, gradient exploding might occur\n",
        "        hidden = self.activation(hidden) if activate else hidden\n",
        "        output = self.outweight(hidden)\n",
        "        return output, hidden"
      ],
      "metadata": {
        "id": "HHEn3YwJeIus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training**"
      ],
      "metadata": {
        "id": "mduw-vjHf8ek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = WordDataSet(word=word)\n",
        "rnn = VanillaRNN(in_size=ds.vec_size, hidden_size=3, out_size=ds.vec_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "e_cnt     = 100\n",
        "optimizer     = optim.SGD(rnn.parameters(), lr = 0.1, momentum=0.9)"
      ],
      "metadata": {
        "id": "vnI-9WmgeI_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CLIP_GRAD = True\n",
        "VERBOSE = False"
      ],
      "metadata": {
        "id": "lnnyeHu6hVgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed()\n",
        "\n",
        "for epoch in range(e_cnt):\n",
        "    hh = torch.zeros(rnn.hidden.in_features)\n",
        "    loss = .0\n",
        "    optimizer.zero_grad()\n",
        "    for sample, next_sample in ds:\n",
        "        x = ds.get_one_hot(sample).unsqueeze_(0)\n",
        "        target = torch.LongTensor([next_sample])\n",
        "        \n",
        "        y, hh = rnn(x, hh)\n",
        "        loss += criterion(y, target)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(loss.data.item())\n",
        "        if CLIP_GRAD: print(\"Clip gradient : \", torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=5))\n",
        "    else:\n",
        "        if CLIP_GRAD: torch.nn.utils.clip_grad_norm_(rnn.parameters(), max_norm=1)\n",
        "\n",
        "    if VERBOSE:\n",
        "      print(\"Params : \")\n",
        "      num_params = 0\n",
        "      for item in rnn.parameters():\n",
        "          num_params += 1\n",
        "          print(item.grad)\n",
        "      print(\"NumParams :\", num_params)\n",
        "      print(\"Optimizer call\")\n",
        "\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OD5i0HO8eJHe",
        "outputId": "8be0234f-dc0d-4139-f217-5b7caa0ea392"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "71.73495483398438\n",
            "Clip gradient :  tensor(5.1487)\n",
            "55.152652740478516\n",
            "Clip gradient :  tensor(5.7516)\n",
            "38.997867584228516\n",
            "Clip gradient :  tensor(9.1299)\n",
            "31.139690399169922\n",
            "Clip gradient :  tensor(8.7716)\n",
            "25.971769332885742\n",
            "Clip gradient :  tensor(9.2260)\n",
            "25.782806396484375\n",
            "Clip gradient :  tensor(13.4613)\n",
            "23.939054489135742\n",
            "Clip gradient :  tensor(7.9822)\n",
            "22.67205238342285\n",
            "Clip gradient :  tensor(11.9995)\n",
            "19.75225257873535\n",
            "Clip gradient :  tensor(6.3413)\n",
            "21.81279754638672\n",
            "Clip gradient :  tensor(10.9281)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rnn.eval()\n",
        "hh = torch.zeros(rnn.hidden.in_features)\n",
        "id = 0\n",
        "softmax  = nn.Softmax(dim=1)\n",
        "predword = ds.get_char_by_id(id)\n",
        "for c in enumerate(word[:-1]):\n",
        "    x = ds.get_one_hot(id).unsqueeze_(0)\n",
        "    y, hh = rnn(x, hh)\n",
        "    y = softmax(y)\n",
        "    m, id = torch.max(y, 1)\n",
        "    id = id.data[0]\n",
        "    predword += ds.get_char_by_id(id)\n",
        "print ('Prediction:\\t' , predword)\n",
        "print(\"Original:\\t\", word)\n",
        "assert(predword == word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "5Fwoy8-EeJQe",
        "outputId": "b97f06b0-6e1f-429f-a89f-1c491dff69f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction:\t oaseqweqweqweqweqweqweqweq\n",
            "Original:\t ololoasdasddqweqw123456789\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-209-30593525f0ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Prediction:\\t'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mpredword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Original:\\t\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredword\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LSTM model**\n",
        "\n",
        "One can observe that vanilla RNN is incapable of remembering long sequences. That's where LSTM and GRU join the game. The intuition behind LSTM is that on each step the model computes the extent to which it should remember (or forget) the history of previous steps and how much attention it should pay to the current input.\n",
        "\n",
        "Basically, LSTM has one extra output on each step called candidate gate. This one prevents the gradient from vanishing, by having non-zero (or almost non-zero) derivative on each iteration. This is akin to what we have seen in ResNet, only applied to sequential format (e.g., time series or text data).\n",
        "\n",
        "However, the insides of LSTM are a bit more complex. At high level the math can be described as follows:\n",
        "\n",
        "$$\n",
        "  \\begin{pmatrix}\n",
        "  i\\\\\n",
        "  f\\\\\n",
        "  o\\\\\n",
        "  g\n",
        "  \\end{pmatrix} = \n",
        "  \\begin{pmatrix}\n",
        "  \\sigma\\\\\n",
        "  \\sigma\\\\\n",
        "  \\sigma\\\\\n",
        "  \\tanh\n",
        "  \\end{pmatrix} W\n",
        "  \\begin{pmatrix}\n",
        "  h_{t-1}\\\\\n",
        "  x_t\n",
        "  \\end{pmatrix}\\\\\n",
        "  c_t = f \\odot c_{t-1} + i \\odot g\\\\\n",
        "  h_t=o \\odot \\tanh(c_t)\n",
        "$$\n",
        "\n",
        "Elements of the ouput vector stand for input, forget (sometimes referred to as remember), output and candidate gates (the latter sometimes is denoted by $c'_t$)\n",
        "\n",
        "It is convenient to write the operations inside LSTM in concatenated matrix form (I forgot the correct term for the case when single matrix consists of several submatrices), with corresponding functions apllied afterwards."
      ],
      "metadata": {
        "id": "m70yhne4ibbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMBase(nn.Module):\n",
        "\n",
        "  def __init__(self, in_size, hidden_size, proj_size=1, bias=True):\n",
        "    super(LSTMBase, self).__init__()\n",
        "\n",
        "    self.input = in_size\n",
        "    self.hidden = hidden_size if proj_size == 1 else proj_size\n",
        "    self.cell = hidden_size\n",
        "    self.out = proj_size\n",
        "\n",
        "    # the model contains 2 linear layers,\n",
        "    # applied to hidden and ordinary inputs respectively\n",
        "    self.Wx = nn.Linear(self.input, self.cell * 4, bias=bias)\n",
        "    self.Wh = nn.Linear(self.hidden, self.cell * 4, bias=bias)\n",
        "    self.sigma = nn.Sigmoid()\n",
        "    self.tanh = nn.Tanh()\n",
        "\n",
        "    # perform projection of the hidden state,\n",
        "    # if specified\n",
        "    self.proj = nn.Sequential() if proj_size == 1\\\n",
        "    else nn.Linear(hidden_size, proj_size, bias=bias)\n",
        "    self.weights_init()\n",
        "\n",
        "  def weights_init(self):\n",
        "    # weight initialization\n",
        "    std = np.sqrt(1 / self.cell) if self.cell > 0 else 0\n",
        "    for weight in self.parameters():\n",
        "      weight.data.uniform_(-std, std)\n",
        "\n",
        "  def forward(self, x, hc):\n",
        "    h_0, c_0 = hc\n",
        "\n",
        "    gates = self.Wx(x) + self.Wh(h_0)\n",
        "    i, f, o, g = gates.chunk(4, dim=-1)\n",
        "    i, f, o = map(self.sigma, (i, f, o))\n",
        "    g = self.tanh(g)\n",
        "\n",
        "    c = f * c_0 + i * g\n",
        "    h = o * self.tanh(c)\n",
        "    out = self.proj(h)\n",
        "\n",
        "    # different ouputs and hiddens\n",
        "    # make sense when several LSTMBases are\n",
        "    # stacked together\n",
        "    return out, (out, c)"
      ],
      "metadata": {
        "id": "7QA4dotLeJYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "\n",
        "  def __init__(self, in_size, hidden_size, proj_size=1, num_layers=1, bias=True):\n",
        "    super(LSTM, self).__init__()\n",
        "    layer_params = []\n",
        "    self.input = in_size\n",
        "    self.hidden = hidden_size if proj_size == 1 else proj_size\n",
        "    self.cell = hidden_size\n",
        "\n",
        "    layers = []\n",
        "\n",
        "    for layer in range(num_layers):\n",
        "      layer_input = in_size if layer == 0 else self.hidden\n",
        "      layers += [LSTMBase(layer_input, self.cell, self.hidden, bias=bias)]\n",
        "\n",
        "    self.cells = nn.ModuleList(layers)\n",
        "\n",
        "  def forward(self, x, hx):\n",
        "    for cell in self.cells:\n",
        "      x, hx = cell(x, hx)\n",
        "    return x, hx"
      ],
      "metadata": {
        "id": "fn03YZZaZ06r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = WordDataSet(word=word)\n",
        "lstm = LSTM(in_size=ds.vec_size, hidden_size=10, proj_size=ds.vec_size, num_layers=2)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "e_cnt     = 1000\n",
        "optimizer = optim.Adam(lstm.parameters(), lr=1e-3, weight_decay=5e-4)"
      ],
      "metadata": {
        "id": "vl1IwiEFo3kx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = WordDataSet(word=word)\n",
        "lstm = LSTM(in_size=ds.vec_size, hidden_size=40, proj_size=ds.vec_size, num_layers=2)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "e_cnt     = 1000\n",
        "optimizer = optim.Adam(lstm.parameters(), lr = 1e-3, weight_decay=1e-4)"
      ],
      "metadata": {
        "id": "4BJmkUKRxWRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDl8O_h0HANH",
        "outputId": "b6cd6e35-e641-4995-c915-fd69698c83b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTM(\n",
              "  (cells): ModuleList(\n",
              "    (0): LSTMBase(\n",
              "      (Wx): Linear(in_features=17, out_features=160, bias=True)\n",
              "      (Wh): Linear(in_features=17, out_features=160, bias=True)\n",
              "      (sigma): Sigmoid()\n",
              "      (tanh): Tanh()\n",
              "      (proj): Linear(in_features=40, out_features=17, bias=True)\n",
              "    )\n",
              "    (1): LSTMBase(\n",
              "      (Wx): Linear(in_features=17, out_features=160, bias=True)\n",
              "      (Wh): Linear(in_features=17, out_features=160, bias=True)\n",
              "      (sigma): Sigmoid()\n",
              "      (tanh): Tanh()\n",
              "      (proj): Linear(in_features=40, out_features=17, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 233
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed()\n",
        "\n",
        "for epoch in range(e_cnt):\n",
        "    hh = torch.zeros(lstm.hidden)\n",
        "    cc = torch.zeros(lstm.cell)\n",
        "\n",
        "    loss = .0\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for sample, next_sample in ds:\n",
        "        x = ds.get_one_hot(sample).unsqueeze_(0)\n",
        "        target = torch.LongTensor([next_sample])\n",
        "\n",
        "        y, (hh, cc) = lstm(x, (hh, cc))\n",
        "        # note the loss values\n",
        "        loss += criterion(y, target)\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print('Loss:\\t', loss.data.item())\n",
        "        if CLIP_GRAD: print(\"Clip gradient : \", torch.nn.utils.clip_grad_norm_(lstm.parameters(), max_norm=5))\n",
        "    else:\n",
        "        if CLIP_GRAD: torch.nn.utils.clip_grad_norm_(lstm.parameters(), max_norm=1)\n",
        "\n",
        "    if VERBOSE:\n",
        "      print(\"Params : \")\n",
        "      num_params = 0\n",
        "      for item in lstm.parameters():\n",
        "          num_params += 1\n",
        "          print(item.grad)\n",
        "      print(\"NumParams :\", num_params)\n",
        "      print(\"Optimizer call\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVDAjMjAeJgX",
        "outputId": "0a22d818-2ce3-4d3d-86d4-729e760adce4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss:\t 70.54817962646484\n",
            "Clip gradient :  tensor(2.6407)\n",
            "Loss:\t 69.99776458740234\n",
            "Clip gradient :  tensor(2.4340)\n",
            "Loss:\t 69.34283447265625\n",
            "Clip gradient :  tensor(2.2860)\n",
            "Loss:\t 68.42424774169922\n",
            "Clip gradient :  tensor(1.9109)\n",
            "Loss:\t 67.38847351074219\n",
            "Clip gradient :  tensor(2.4701)\n",
            "Loss:\t 65.09686279296875\n",
            "Clip gradient :  tensor(5.6103)\n",
            "Loss:\t 56.96276092529297\n",
            "Clip gradient :  tensor(16.7404)\n",
            "Loss:\t 53.17577362060547\n",
            "Clip gradient :  tensor(32.7584)\n",
            "Loss:\t 51.35645294189453\n",
            "Clip gradient :  tensor(55.3093)\n",
            "Loss:\t 48.9168586730957\n",
            "Clip gradient :  tensor(10.5449)\n",
            "Loss:\t 47.04542541503906\n",
            "Clip gradient :  tensor(49.0878)\n",
            "Loss:\t 45.27079772949219\n",
            "Clip gradient :  tensor(12.3696)\n",
            "Loss:\t 43.58772659301758\n",
            "Clip gradient :  tensor(21.7121)\n",
            "Loss:\t 41.91994857788086\n",
            "Clip gradient :  tensor(11.0562)\n",
            "Loss:\t 40.462646484375\n",
            "Clip gradient :  tensor(98.0084)\n",
            "Loss:\t 39.14867401123047\n",
            "Clip gradient :  tensor(118.2272)\n",
            "Loss:\t 37.7833137512207\n",
            "Clip gradient :  tensor(60.2380)\n",
            "Loss:\t 36.635860443115234\n",
            "Clip gradient :  tensor(87.9479)\n",
            "Loss:\t 35.441226959228516\n",
            "Clip gradient :  tensor(52.7811)\n",
            "Loss:\t 34.61128234863281\n",
            "Clip gradient :  tensor(162.8190)\n",
            "Loss:\t 33.48784255981445\n",
            "Clip gradient :  tensor(103.8609)\n",
            "Loss:\t 32.45737075805664\n",
            "Clip gradient :  tensor(70.1700)\n",
            "Loss:\t 31.46455192565918\n",
            "Clip gradient :  tensor(34.1114)\n",
            "Loss:\t 30.503713607788086\n",
            "Clip gradient :  tensor(22.0123)\n",
            "Loss:\t 29.54389190673828\n",
            "Clip gradient :  tensor(8.3952)\n",
            "Loss:\t 28.5792236328125\n",
            "Clip gradient :  tensor(14.0682)\n",
            "Loss:\t 27.599336624145508\n",
            "Clip gradient :  tensor(32.9089)\n",
            "Loss:\t 30.975282669067383\n",
            "Clip gradient :  tensor(822.2152)\n",
            "Loss:\t 26.401649475097656\n",
            "Clip gradient :  tensor(87.3954)\n",
            "Loss:\t 25.599279403686523\n",
            "Clip gradient :  tensor(44.4053)\n",
            "Loss:\t 25.009437561035156\n",
            "Clip gradient :  tensor(84.5922)\n",
            "Loss:\t 24.2582950592041\n",
            "Clip gradient :  tensor(12.2115)\n",
            "Loss:\t 23.6185302734375\n",
            "Clip gradient :  tensor(29.2011)\n",
            "Loss:\t 22.95220947265625\n",
            "Clip gradient :  tensor(8.6601)\n",
            "Loss:\t 22.294198989868164\n",
            "Clip gradient :  tensor(11.9724)\n",
            "Loss:\t 21.6317138671875\n",
            "Clip gradient :  tensor(6.0964)\n",
            "Loss:\t 20.973209381103516\n",
            "Clip gradient :  tensor(6.3558)\n",
            "Loss:\t 20.318700790405273\n",
            "Clip gradient :  tensor(6.2587)\n",
            "Loss:\t 19.6695499420166\n",
            "Clip gradient :  tensor(5.8407)\n",
            "Loss:\t 19.02850914001465\n",
            "Clip gradient :  tensor(5.6617)\n",
            "Loss:\t 18.398595809936523\n",
            "Clip gradient :  tensor(5.5882)\n",
            "Loss:\t 17.780208587646484\n",
            "Clip gradient :  tensor(5.5223)\n",
            "Loss:\t 17.17136001586914\n",
            "Clip gradient :  tensor(5.4520)\n",
            "Loss:\t 16.570655822753906\n",
            "Clip gradient :  tensor(5.3760)\n",
            "Loss:\t 15.978684425354004\n",
            "Clip gradient :  tensor(5.2922)\n",
            "Loss:\t 15.397266387939453\n",
            "Clip gradient :  tensor(5.2036)\n",
            "Loss:\t 14.828108787536621\n",
            "Clip gradient :  tensor(5.1145)\n",
            "Loss:\t 14.272409439086914\n",
            "Clip gradient :  tensor(5.0266)\n",
            "Loss:\t 13.731474876403809\n",
            "Clip gradient :  tensor(4.9408)\n",
            "Loss:\t 13.206918716430664\n",
            "Clip gradient :  tensor(4.8578)\n",
            "Loss:\t 12.700304985046387\n",
            "Clip gradient :  tensor(4.7778)\n",
            "Loss:\t 12.212831497192383\n",
            "Clip gradient :  tensor(4.7011)\n",
            "Loss:\t 11.746127128601074\n",
            "Clip gradient :  tensor(4.6261)\n",
            "Loss:\t 11.302959442138672\n",
            "Clip gradient :  tensor(4.5509)\n",
            "Loss:\t 10.882820129394531\n",
            "Clip gradient :  tensor(4.4758)\n",
            "Loss:\t 10.483576774597168\n",
            "Clip gradient :  tensor(4.3988)\n",
            "Loss:\t 10.103734016418457\n",
            "Clip gradient :  tensor(4.3195)\n",
            "Loss:\t 9.742008209228516\n",
            "Clip gradient :  tensor(4.2391)\n",
            "Loss:\t 9.397298812866211\n",
            "Clip gradient :  tensor(4.1568)\n",
            "Loss:\t 9.068697929382324\n",
            "Clip gradient :  tensor(4.0722)\n",
            "Loss:\t 8.755413055419922\n",
            "Clip gradient :  tensor(3.9862)\n",
            "Loss:\t 8.456713676452637\n",
            "Clip gradient :  tensor(3.8997)\n",
            "Loss:\t 8.171899795532227\n",
            "Clip gradient :  tensor(3.8133)\n",
            "Loss:\t 7.900295734405518\n",
            "Clip gradient :  tensor(3.7276)\n",
            "Loss:\t 7.641246795654297\n",
            "Clip gradient :  tensor(3.6432)\n",
            "Loss:\t 7.394101142883301\n",
            "Clip gradient :  tensor(3.5601)\n",
            "Loss:\t 7.158210754394531\n",
            "Clip gradient :  tensor(3.4786)\n",
            "Loss:\t 6.932923793792725\n",
            "Clip gradient :  tensor(3.3989)\n",
            "Loss:\t 6.717562675476074\n",
            "Clip gradient :  tensor(3.3208)\n",
            "Loss:\t 6.511433124542236\n",
            "Clip gradient :  tensor(3.2444)\n",
            "Loss:\t 6.313930511474609\n",
            "Clip gradient :  tensor(3.1695)\n",
            "Loss:\t 6.124783039093018\n",
            "Clip gradient :  tensor(3.0957)\n",
            "Loss:\t 5.943741798400879\n",
            "Clip gradient :  tensor(3.0215)\n",
            "Loss:\t 5.770274639129639\n",
            "Clip gradient :  tensor(2.9479)\n",
            "Loss:\t 5.603899955749512\n",
            "Clip gradient :  tensor(2.8763)\n",
            "Loss:\t 5.444061279296875\n",
            "Clip gradient :  tensor(2.8074)\n",
            "Loss:\t 5.2901716232299805\n",
            "Clip gradient :  tensor(2.7417)\n",
            "Loss:\t 5.141602993011475\n",
            "Clip gradient :  tensor(2.6804)\n",
            "Loss:\t 4.997670650482178\n",
            "Clip gradient :  tensor(2.6245)\n",
            "Loss:\t 4.857606887817383\n",
            "Clip gradient :  tensor(2.5755)\n",
            "Loss:\t 4.720534324645996\n",
            "Clip gradient :  tensor(2.5353)\n",
            "Loss:\t 4.585441589355469\n",
            "Clip gradient :  tensor(2.5056)\n",
            "Loss:\t 4.451180934906006\n",
            "Clip gradient :  tensor(2.4880)\n",
            "Loss:\t 4.316493511199951\n",
            "Clip gradient :  tensor(2.4832)\n",
            "Loss:\t 4.1801066398620605\n",
            "Clip gradient :  tensor(2.4892)\n",
            "Loss:\t 4.0409369468688965\n",
            "Clip gradient :  tensor(2.4993)\n",
            "Loss:\t 3.898449420928955\n",
            "Clip gradient :  tensor(2.5019)\n",
            "Loss:\t 3.753023386001587\n",
            "Clip gradient :  tensor(2.4859)\n",
            "Loss:\t 3.605733633041382\n",
            "Clip gradient :  tensor(2.4496)\n",
            "Loss:\t 3.4573256969451904\n",
            "Clip gradient :  tensor(2.4021)\n",
            "Loss:\t 3.307497024536133\n",
            "Clip gradient :  tensor(2.3559)\n",
            "Loss:\t 3.1563098430633545\n",
            "Clip gradient :  tensor(2.3147)\n",
            "Loss:\t 3.0072035789489746\n",
            "Clip gradient :  tensor(2.2651)\n",
            "Loss:\t 2.86531662940979\n",
            "Clip gradient :  tensor(2.1998)\n",
            "Loss:\t 2.7318267822265625\n",
            "Clip gradient :  tensor(2.1307)\n",
            "Loss:\t 2.605452537536621\n",
            "Clip gradient :  tensor(2.0654)\n",
            "Loss:\t 2.488156795501709\n",
            "Clip gradient :  tensor(1.9982)\n",
            "Loss:\t 2.382636308670044\n",
            "Clip gradient :  tensor(1.9277)\n",
            "Loss:\t 2.2860586643218994\n",
            "Clip gradient :  tensor(1.8617)\n",
            "Loss:\t 2.195971727371216\n",
            "Clip gradient :  tensor(1.8013)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lstm.eval()\n",
        "\n",
        "hh = torch.zeros(lstm.hidden)\n",
        "cc = torch.zeros(lstm.cell)\n",
        "id = 0\n",
        "softmax  = nn.Softmax(dim=-1)\n",
        "predword = ds.get_char_by_id(id)\n",
        "\n",
        "for c in enumerate(word[:-1]):\n",
        "    x = ds.get_one_hot(id).unsqueeze_(0)\n",
        "    y, (hh, cc) = lstm(x, (hh, cc))\n",
        "    y = softmax(y)\n",
        "    m, id = torch.max(y, 1)\n",
        "    id = id.data[0]\n",
        "    predword += ds.get_char_by_id(id)\n",
        "\n",
        "print ('Prediction:\\t' , predword)\n",
        "print(\"Original:\\t\", word)\n",
        "assert(predword == word)"
      ],
      "metadata": {
        "id": "fivedDRReJoB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9201db4f-b244-4368-f438-f3475eb0699a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction:\t ololoasdasddqweqw123456789\n",
            "Original:\t ololoasdasddqweqw123456789\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is funny how the model only managed to overfit on the sequence when trained for 1000 epochs with tuned optimizer parameters. Also, the model only started converging when I substituted SGD with Adam (the impact was huge).\n",
        "I do not use BN"
      ],
      "metadata": {
        "id": "qvbrnD6Hqmhs"
      }
    }
  ]
}